{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS4721 Spring 2015: Homework 2\n",
    "\n",
    "Daniel M. Sheehan - dms2203@columbia.edu\n",
    "Discussants: tudor, jade\n",
    "## Problem 1 - Skipped (we can pick 2 of the first 3 problems) \n",
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![prob2](img/problem_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a. The classifier based on the generative model where class conditional distributions are **multi-variate Gaussian distributions with a mixed covariance** equal to the identity matrix *I*. Assume MLE is used for parameter estimation.\n",
    "\n",
    "\t* **Centering**: Does not have an effect on the classification rate because we're just changing the origin. Its shifted in space but relative positions are the same. We're effecting the model, but not the classifier. But the position of the sep. hyperplane has moved so the model is different. \n",
    "\t\n",
    "\t* **Standardization**: There is no effect because since they have covariance then the classifier is a straight line. We affect the model, distance but not the classification rate.   \n",
    "\t\n",
    "* b.  The **1-NN classifer using Euclidean distance**.\n",
    "\n",
    "\t* **Centering**: Centering will not effect 1-NN because the distance between the points will not be altered.\n",
    "\t* **Standardization**: Standarization will effect the 1-NN because the distance between the features will have changed. \n",
    "\t\n",
    "* c. The **greedy decision tree learning algorithm** with axis-aligned splits.\n",
    "\n",
    "\t* **Centering**: Centering will affect the classifier because it cannot return the global optimal decision tree.\n",
    "\t* **Standardization**: If centering affects it in this instance, standardization also does. \n",
    "\n",
    "* d. **Empirical Risk Minimization**\t\n",
    "\n",
    "\t* **Centering**: Not certain this can be achieved b/c of complexity. \n",
    "\t* **Standardization**: ibid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "![prob3](img/problem_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "$ \\sum v = \\lambda v $\n",
    "\n",
    "####a) eigenvalues of $ \\sum  + \\sigma^2 I $\n",
    "\n",
    "$ \\sum = \\begin{bmatrix} \n",
    "\\sigma_1^2 & \\sigma_{12} \\\\\n",
    "\\sigma_{21} & \\sigma_2^2 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$ \\sigma^2 I + \\sum = \\begin{bmatrix} \n",
    "\\sigma_1^2 & \\sigma_{12} \\\\\n",
    "\\sigma_{21} & \\sigma_2^2 \n",
    "\\end{bmatrix} + \\begin{bmatrix} \n",
    "\\sigma^2 & 0 \\\\\n",
    "0 & \\sigma^2 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$ \\sum \\leftarrow orginal \\ covariance $\n",
    "\n",
    "$ \\sigma^2 \\begin{bmatrix} \n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "\\sigma^2 & 0 \\\\\n",
    "0 & \\sigma^2 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$ \\sum + \\sigma^2I =  \\begin{bmatrix} \n",
    "\\sigma_1^2 + \\sigma^2 & \\sigma_{12} \\\\\n",
    "\\sigma_{21} & \\sigma_2^2 + \\sigma^2\n",
    "\\end{bmatrix} $\n",
    "\n",
    "---\n",
    "$ ( \\sum + \\sigma^2I) v' = \\lambda'v'$\n",
    "\n",
    "---\n",
    "\n",
    "$ \\sum v' + \\sigma^2 I v' = \\lambda'v' $\n",
    "\n",
    "$ \\sum v' = \\lambda'v' - \\sigma^2Iv' $\n",
    "\n",
    "---\n",
    "$ \\sum v' = (\\lambda' - \\sigma^2)v' $\n",
    "\n",
    "---\n",
    "\n",
    "$ v' \\ eigenvector \\ of \\sum $\n",
    "\n",
    "$ x' - \\sigma^2 \\ eigenvalue \\ of \\sum $\n",
    "\n",
    "$ \\lambda' - \\sigma^2 = \\lambda $\n",
    "\n",
    "---\n",
    "$ \\lambda' = \\lambda + \\sigma^2 $\n",
    "\n",
    "---\n",
    "\n",
    "####b) eigenvalues of $ (\\sum  + \\sigma^2 I)^{-1} $\n",
    "\n",
    "$ ( \\sum + \\sigma^2I )^{-1} v' = \\lambda'v'        $\n",
    "\n",
    "$ v' = \\lambda' ( \\sum + \\sigma^2 I ) v' $ \n",
    "\n",
    "$ \\dfrac{1}{\\lambda'} v' = ( \\sum + \\sigma^2 I ) v' $\n",
    "\n",
    "$ = \\sum v' + \\sigma^2 v' $\n",
    "\n",
    "$ \\sum v' = \\dfrac{1}{\\lambda'} v'  - \\sigma^2 v' $\n",
    "\n",
    "$ \\sum v' = (\\dfrac{1}{\\lambda'}  - \\sigma^2 ) v' , \\leftarrow this \\ here \\ is \\ \\lambda$\n",
    "\n",
    "$ \\lambda = \\dfrac{1}{\\lambda'} - \\sigma^2 $\n",
    "\n",
    "$ \\lambda + \\sigma^2 = \\dfrac{1}{\\lambda'} $\n",
    "\n",
    "---\n",
    "$ \\lambda' = \\dfrac{1}{\\lambda + \\sigma^2} $\n",
    "\n",
    "---\n",
    "\n",
    "####c) \n",
    "\n",
    "$ RR = \\sum + \\sigma^2 I $\n",
    "\n",
    "$ R^{-1}R = I, \\ \\ RR^{-1} = I $\n",
    "\n",
    "$ \\sum v_1 = \\lambda_1 v_1 $ \n",
    "\n",
    "$ E(X) = 0 $\n",
    "\n",
    "$ Y = v_1^T R^{-1} X $\n",
    "\n",
    "$ \\mu_Y = E(Y) = E(v_1^T, R^{-1} X) $\n",
    "\n",
    "$ = v_1^T R^{-1} E(X), \\leftarrow which \\ is \\ 0 $\n",
    "\n",
    "$ Var(Y) = E[Y^2] $\n",
    "\n",
    "$ = E [(v_1^T R^{-1} X) ( v_1^T R^{-1} X)^T ] $\n",
    "\n",
    "$ = E [v_1^TR^{-1} X X^T R^{-T} v_1 ]    $\n",
    "\n",
    "\n",
    "$ = v_1^TR^{-1} E( X X^T)  R^{-T} v_1     $\n",
    "\n",
    "$ = v_1^T R^{-1} \\sum R^{-T} v_1 $\n",
    "\n",
    "$ RR = \\sum + \\sigma^2I, \\ \\ \\ \\ \\ \\  R = R^T \\rightarrow R^{-1} = (R^{-1})^T $\n",
    "\n",
    "$R^{-1}RR = R^{-1} \\sum + \\sigma^2 R^{-1} I $\n",
    "\n",
    "^ $ \\ above \\ is \\ IR, \\ which \\ is \\ R $\n",
    "\n",
    "$ R = R^{-1}\\sum + \\sigma^2 R^{-1}, \\ \\ \\ x(R^{-1})^T = R^{-1}$\n",
    "\n",
    "$ RR^{-1} = R^{-1} \\sum R^{-T} + \\sigma^2 R^{-1} R^{-1} $\n",
    "\n",
    "$ I = R^{-1} \\sum R^{-T} + \\sigma^2 R^{-1} R^{-1} $\n",
    "\n",
    "$R^{-1} \\sum R^{-T} = I - \\sigma^2 R{-1} R^{-1} $\n",
    "\n",
    "$Var(Y) = v_1^T(I - \\sigma^2 R^{-1} R^{-1} ) v_1 $\n",
    "\n",
    "$ = v_1^T v_1 - \\sigma^2 v_1^T R^{-1} R^{-1}  v_1 $\n",
    "\n",
    "$ = 1 - \\sigma^2 v_1^T R^{-1} R^{-1} v_1 $\n",
    "\n",
    "$ \\sum v_1 = \\lambda_1 v_1, \\leftarrow \\sum = RR - \\sigma^2I $ \n",
    "\n",
    "$ RR = \\sum + \\sigma^2 I $\n",
    "\n",
    "$ RR v_1 = (\\lambda_1 + \\sigma^2 ) v_1 $\n",
    "multiply both sides by $R^{-1}$\n",
    "\n",
    "$ R^{-1}RR v_1 = R^{-1}(\\lambda_1 + \\sigma^2 ) v_1 $\n",
    "\n",
    "^ $ is \\ I $\n",
    "\n",
    "$ Rv_1  = (\\lambda_1 + \\sigma^2) R^{-1} v_1 $\n",
    "\n",
    "---\n",
    "$ R^{-1}v_1 = \\dfrac{R v_1}{\\lambda_1 + \\sigma^2} $\n",
    "\n",
    "---\n",
    "\n",
    "transpose\n",
    "\n",
    "$ (R^{-1} v_1)^T = V_1^T R^{-T} = v_1^T R^{-1} = \\dfrac{v_1^T R^T}{\\lambda_1 + \\sigma^2} $\n",
    "\n",
    "---\n",
    "$ \\rightarrow v_1^T R^{-1} = \\dfrac{v_1^T R}{\\lambda_1 + \\sigma^2} $\n",
    "\n",
    "---\n",
    "\n",
    "$ var(Y) = 1 - \\sigma^2 v_1^T R^{-1} R^{-1} v_1 $\n",
    "\n",
    "$ = 1 - \\sigma^2 (\\dfrac{v_1^T R}{\\lambda_1 + \\sigma^2})(\\dfrac{Rv_1}{\\lambda_1 + \\sigma^2}) $\n",
    "\n",
    "$ = 1 - \\dfrac{\\sigma^2}{(\\lambda_1 + \\sigma^2)^2} v_1^T ( RR ) v_1  $\n",
    "\n",
    "$ \\ \\ \\ \\  note: RR = \\sum + \\sigma^2 I $\n",
    "\n",
    "$ = 1 - \\dfrac{\\sigma^2}{(\\lambda_1 + \\sigma^2)^2} [v_1^T \\sum v_1 + \\sigma^2 v_1^T v_1$\n",
    "\n",
    "$ \\ \\ \\ v_1^Tv_1 \\ is \\ 1 $\n",
    "\n",
    "$ = 1 - \\dfrac{\\sigma^2}{(\\lambda_1 + \\sigma^2)^2} (\\lambda_1 + \\sigma^2)    $\n",
    "\n",
    "---\n",
    "$ = 1 - \\dfrac{\\sigma^2}{\\lambda_1 + \\sigma^2}  $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "![prob4](img/problem_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 5. Big Perceptron accuracy: 0.912894492741\n",
      "Model: 2. Logistic accuracy: 0.918437344953\n",
      "Model: TEST for. RandomForest accuracy: 0.938654356408\n",
      "Model: 3. Gauss 1 accuracy: 0.868188191643\n",
      "Model: 4. Gauss 2 accuracy: 0.806716353517\n",
      "Model: 1. Perceptron accuracy: 0.918415773238\n",
      "Model: 6. Big Logistic accuracy: 0.923985590094\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = loadmat('spam_fixed.mat')\n",
    "\n",
    "Ytrain = data['labels'].flatten()\n",
    "Xtrain = data['data']\n",
    "Ytest  = data['testlabels'].flatten()\n",
    "Xtest  = data['testdata']\n",
    "\n",
    "class Perceptron(object):\n",
    "\tdef fit(self, X, Y):\n",
    "\t\tN, D = X.shape\n",
    "\t\tV = [np.zeros(D)]\n",
    "\t\tC = [0]\n",
    "\t\tfor t in xrange(64):\n",
    "\t\t\tfor i in xrange(N):\n",
    "\t\t\t\tv = V[-1]\n",
    "\t\t\t\ty_hat_i = np.sign(v.dot(X[i]))\n",
    "\t\t\t\tif y_hat_i == Y[i]:\n",
    "\t\t\t\t\tC[-1] = C[-1] + 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnew_v = v + Y[i]*X[i]\n",
    "\t\t\t\t\tV.append(new_v)\n",
    "\t\t\t\t\tC.append(1)\n",
    "\t\t\t\t\n",
    "\t\tself.v = np.zeros(D)\n",
    "\t\ttotal_votes = 0\n",
    "\t\tfor c, v in zip(C[1:], V[1:]):\n",
    "\t\t\tself.v += c*v\n",
    "\t\t\ttotal_votes += c\n",
    "\t\tself.v /= total_votes\n",
    "\n",
    "\tdef score(self, X, Y):\n",
    "\t\tP = np.sign(X.dot(self.v))\n",
    "\t\treturn np.mean(P == Y)\n",
    "\n",
    "class BigPerceptron(object):\n",
    "\tdef fit(self, X, Y):\n",
    "\t\tX2 = transform(X)\n",
    "\t\tself.model = Perceptron()\n",
    "\t\tself.model.fit(X2, Y)\n",
    "\n",
    "\tdef score(self, X, Y):\n",
    "\t\tX2 = transform(X)\n",
    "\t\treturn self.model.score(X2, Y)\n",
    "\n",
    "class Gauss1(object):\n",
    "\t# Ax = b\n",
    "\t# x = inv_A * b\n",
    "\t# solve(A, b)\n",
    "\tdef fit(self, X, Y):\n",
    "\t\t# w = inv_cov * (mu1 - mu0)\n",
    "\t\t# b = 0.5*(mu1 + mu0) * inv_cov * (mu0 - mu1) + log(pi1/pi0)\n",
    "\t\tcov = np.cov(X.T)\n",
    "\t\tidx1 = np.where(Y == 1)[0]\n",
    "\t\tidx0 = np.where(Y == -1)[0]\n",
    "\t\tmu0 = X[idx0, :].mean(axis=0)\n",
    "\t\tmu1 = X[idx1, :].mean(axis=0)\n",
    "\t\tself.w = np.linalg.solve(cov, mu1 - mu0)\n",
    "\t\tN = len(Y)\n",
    "\t\tpi1 = float(len(idx1)) / N\n",
    "\t\tpi0 = float(len(idx0)) / N\n",
    "\t\tself.b = -0.5*(mu0 + mu1).dot(self.w) + np.log(pi1/pi0)\n",
    "\n",
    "\tdef score(self, X, Y):\n",
    "\t\tP = np.sign(X.dot(self.w) + self.b)\n",
    "\t\treturn np.mean(P == Y)\n",
    "\n",
    "class Gauss2(object):\n",
    "\tdef fit(self, X, Y):\n",
    "\t\tidx1 = np.where(Y == 1)[0]\n",
    "\t\tidx0 = np.where(Y == -1)[0]\n",
    "\n",
    "\t\tx0 = X[idx0, :]\n",
    "\t\tx1 = X[idx1, :]\n",
    "\t\tmu0 = x0.mean(axis=0)\n",
    "\t\tmu1 = x1.mean(axis=0)\n",
    "\t\tcov0 = np.cov(x0.T)\n",
    "\t\tcov1 = np.cov(x1.T)\n",
    "\n",
    "\t\tself.A = 0.5*(np.linalg.inv(cov0) - np.linalg.inv(cov1))\n",
    "\t\ticov0mu0 = np.linalg.solve(cov0, mu0)\n",
    "\t\ticov1mu1 = np.linalg.solve(cov1, mu1)\n",
    "\t\tself.w = icov1mu1 - icov0mu0\n",
    "\t\tN = len(Y)\n",
    "\t\tpi1 = float(len(idx1)) / N\n",
    "\t\tpi0 = float(len(idx0)) / N\n",
    "\t\tself.b = 0.5*(mu0.dot(icov0mu0) - mu1.dot(icov1mu1)) + np.log(pi1/pi0)\t\n",
    "\n",
    "\tdef score(self, X, Y):\n",
    "\t\tP = np.sign((X.dot(self.A)*X).sum(axis=1) + X.dot(self.w) + self.b)\n",
    "\t\treturn np.mean(P == Y)\n",
    "\n",
    "def transform(X):\n",
    "\tN, D = X.shape\n",
    "\tX2 = np.zeros((N, 1710))\n",
    "\tX2[:,:D] = X\n",
    "\tj = D\n",
    "\tfor i in xrange(D):\n",
    "\t\tfor k in xrange(D):\n",
    "\t\t\tif i <= k:\n",
    "\t\t\t\tX2[:,j] = X[:,i]*X[:,k]\n",
    "\t\t\t\tj += 1\n",
    "\t# mu = X2.mean(axis=0)\n",
    "\t# std = X2.std(axis=0)\n",
    "\t# X2 = (X2 - mu) / std\n",
    "\treturn X2\n",
    "\n",
    "class BigLogistic(object):\n",
    "\tdef fit(self, X, Y):\n",
    "\t\tX2 = transform(X)\n",
    "\t\tself.model = LogisticRegression()\n",
    "\t\tself.model.fit(X2, Y)\n",
    "\n",
    "\tdef score(self, X, Y):\n",
    "\t\tX2 = transform(X)\n",
    "\t\treturn self.model.score(X2, Y)\n",
    "\n",
    "def cross_validation(model, X, Y):\n",
    "\t# split the data into 10 parts\n",
    "\tN = len(Y)\n",
    "\tbatchsize = N / 10 + 1\n",
    "\tscores = []\n",
    "\tfor i in xrange(10):\n",
    "\t\t# test on i-th part, train on other 9 parts\n",
    "\t\t# (i + 1)*batchsize\n",
    "\t\tstart = i*batchsize\n",
    "\t\tend = (i*batchsize + batchsize)\n",
    "\t\tXvalid = X[start:end]\n",
    "\t\tYvalid = Y[start:end]\n",
    "\n",
    "\t\tXtrain = np.concatenate([  X[:start] , X[end:] ])\n",
    "\t\tYtrain = np.concatenate([  Y[:start] , Y[end:] ])\n",
    "\n",
    "\t\tmodel.fit(Xtrain, Ytrain)\n",
    "\t\tscores.append(model.score(Xvalid, Yvalid))\n",
    "\treturn np.mean(scores)\n",
    "\n",
    "\n",
    "models = {\n",
    "\t'2. Logistic': LogisticRegression(),\n",
    "\t'TEST for. RandomForest': RandomForestClassifier(),\n",
    "\t'1. Perceptron': Perceptron(),\n",
    "\t'6. Big Logistic': BigLogistic(),\n",
    "\t'5. Big Perceptron': BigPerceptron(),\n",
    "\t'3. Gauss 1': Gauss1(),\n",
    "\t'4. Gauss 2': Gauss2(),\n",
    "}\n",
    "\n",
    "def main():\n",
    "\tfor name, model in models.iteritems():\n",
    "\t\tprint \"Model:\", name, \"accuracy:\", cross_validation(model, Xtrain, Ytrain)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
